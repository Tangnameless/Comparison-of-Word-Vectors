{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.导入需要的工具包"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json, time \n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertConfig, AdamW, get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fef99194f50>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.载入预训练模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)  # 初始化分词器"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c962e97b58f4e40b2695287b658b359"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62ea6f7736b747e9a380194a9a1b1911"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a075705fabc449a2a8e3f43520b33e78"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.数据预处理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**数据示例**    \r\n",
    "may be more genial than ingenious , but it gets the job done—>1  \r\n",
    "用制表符 '\\t' 分割文本和标签 \r\n",
    " \r\n",
    "**利用分词器进行编码**  \r\n",
    "encode仅返回input_ids\r\n",
    "```py\r\n",
    "print(tokenizer.encode('我不喜欢你'))                    #[101, 2769, 679, 1599, 3614, 872, 102]\r\n",
    "```\r\n",
    "encode_plus返回所有编码信息\r\n",
    "- input_ids：是单词在词典中的编码\r\n",
    "- token_type_ids：区分两个句子的编码（上句全为0，下句全为1）\r\n",
    "- attention_mask：指定对哪些词进行self-Attention操作\r\n",
    "\r\n",
    "```py\r\n",
    "sen_code = tokenizer.encode_plus('我不喜欢这世界','我只喜欢你')\r\n",
    "print(sen_code)\r\n",
    "# {\r\n",
    "#   'input_ids': [101, 2769, 679, 1599, 3614, 6821, 686, 4518, 102, 2769, 1372, 1599, 3614, 872, 102], \r\n",
    "#   'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], \r\n",
    "#   'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n",
    "# }\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "input_ids, input_masks  = [], []  # input char ids, attention mask\n",
    "labels = []      # 标签\n",
    "maxlen = 66      \n",
    " \n",
    "df = pd.read_csv('./train.tsv', delimiter='\\t', header=None)\n",
    "for index, sentence in enumerate(df[0]):\n",
    "    # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "    # 根据参数会短则补齐，长则切断\n",
    "    encode_dict = tokenizer.encode_plus(text=sentence,\n",
    "                                        max_length=maxlen, \n",
    "                                        padding='max_length', \n",
    "                                        truncation=True)\n",
    "    \n",
    "    input_ids.append(encode_dict['input_ids'])\n",
    "    input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "labels = list(df[1])\n",
    "\n",
    "input_ids, input_masks = np.array(input_ids), np.array(input_masks)\n",
    "labels = np.array(labels)\n",
    "print(input_ids.shape, input_masks.shape, labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6920, 66) (6920, 66) (6920,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 切分训练集，验证集，测试集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 随机打乱索引\n",
    "idxes = np.arange(input_ids.shape[0])\n",
    "np.random.shuffle(idxes)\n",
    "print(idxes.shape, idxes[:10])\n",
    "\n",
    "\n",
    "# 8:1:1 划分训练集、验证集、测试集\n",
    "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:5536]], input_ids[idxes[5536:6226]], input_ids[idxes[6226:]]\n",
    "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:5536]], input_masks[idxes[5536:6226]], input_masks[idxes[6226:]] \n",
    "\n",
    "y_train, y_valid, y_test = labels[idxes[:5536]], labels[idxes[5536:6226]], labels[idxes[6226:]]\n",
    "\n",
    "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
    "      input_ids_test.shape, y_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6920,) [1166 3851 2611 4407 6763 1024 4097 5390 3909 3041]\n",
      "(5536, 66) (5536,) (690, 66) (690,) (694, 66) (694,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. 加载到PyTorch的DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "BATCH_SIZE = 64  # 如果会出现OOM问题，减小它\n",
    "# 训练集\n",
    "# TensorDataset 可以用来对tensor进行打包。\n",
    "train_data = TensorDataset( torch.LongTensor(input_ids_train), \n",
    "                            torch.LongTensor(input_masks_train), \n",
    "                            torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "# 验证集\n",
    "valid_data = TensorDataset( torch.LongTensor(input_ids_valid), \n",
    "                            torch.LongTensor(input_masks_valid),\n",
    "                            torch.LongTensor(y_valid))\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 测试集（是没有标签的）\n",
    "test_data = TensorDataset( torch.LongTensor(input_ids_test), \n",
    "                           torch.LongTensor(input_masks_test))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. 定义BERT模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, bert_path, classes=2):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = DistilBertConfig.from_pretrained(bert_path)\n",
    "        self.bert = model_class.from_pretrained(bert_path)       # 加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size, classes)    # 直接分类\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask) # [batchsize, len, 768]\n",
    "        out_pool = outputs[0][:,0,:]       # 池化后的输出 [bs, config.hidden_size]\n",
    "        logit = self.fc(out_pool)   # [bs, classes]\n",
    "        return logit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. 实例化BERT模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "\n",
    "# set cuda\n",
    "gpu = 1\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    DEVICE = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "EPOCHS = 5\n",
    "model = Bert_Model(pretrained_weights).to(DEVICE)\n",
    "print(get_parameter_number(model))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total parameters: 66364418, Trainable parameters: 66364418\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. 定义优化器"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 这里给个小提示，一定要加warmup（学习率从0慢慢升上去），要不然你把它去掉试试，基本上收敛不了。"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. 定义训练函数和测试验证函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "    return val_pred\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, y) in enumerate(train_loader):\n",
    "            ids, att, y = ids.to(device), att.to(device), y.to(device)  \n",
    "            y_pred = model(ids, att)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. 开始训练和验证模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# 训练和验证评估\n",
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0017/0087 | Loss 0.5764 | Time 3.0348\n",
      "Epoch 0001 | Step 0034/0087 | Loss 0.4900 | Time 6.0399\n",
      "Epoch 0001 | Step 0051/0087 | Loss 0.4443 | Time 9.0379\n",
      "Epoch 0001 | Step 0068/0087 | Loss 0.4073 | Time 12.0426\n",
      "Epoch 0001 | Step 0085/0087 | Loss 0.3860 | Time 15.0543\n",
      "current acc is 0.8594, best acc is 0.8594\n",
      "time costed = 17.07684s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0017/0087 | Loss 0.2220 | Time 3.0130\n",
      "Epoch 0002 | Step 0034/0087 | Loss 0.2146 | Time 6.0488\n",
      "Epoch 0002 | Step 0051/0087 | Loss 0.2033 | Time 9.0883\n",
      "Epoch 0002 | Step 0068/0087 | Loss 0.1980 | Time 12.1371\n",
      "Epoch 0002 | Step 0085/0087 | Loss 0.1969 | Time 15.1804\n",
      "current acc is 0.8812, best acc is 0.8812\n",
      "time costed = 17.175s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0017/0087 | Loss 0.1079 | Time 3.0570\n",
      "Epoch 0003 | Step 0034/0087 | Loss 0.1083 | Time 6.1206\n",
      "Epoch 0003 | Step 0051/0087 | Loss 0.1043 | Time 9.1950\n",
      "Epoch 0003 | Step 0068/0087 | Loss 0.0989 | Time 12.2661\n",
      "Epoch 0003 | Step 0085/0087 | Loss 0.1040 | Time 15.3312\n",
      "current acc is 0.8783, best acc is 0.8812\n",
      "time costed = 16.25114s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0017/0087 | Loss 0.0494 | Time 3.0685\n",
      "Epoch 0004 | Step 0034/0087 | Loss 0.0514 | Time 6.1437\n",
      "Epoch 0004 | Step 0051/0087 | Loss 0.0541 | Time 9.2355\n",
      "Epoch 0004 | Step 0068/0087 | Loss 0.0578 | Time 12.3325\n",
      "Epoch 0004 | Step 0085/0087 | Loss 0.0610 | Time 15.4215\n",
      "current acc is 0.8754, best acc is 0.8812\n",
      "time costed = 16.34544s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0017/0087 | Loss 0.0428 | Time 3.0878\n",
      "Epoch 0005 | Step 0034/0087 | Loss 0.0428 | Time 6.1922\n",
      "Epoch 0005 | Step 0051/0087 | Loss 0.0448 | Time 9.2992\n",
      "Epoch 0005 | Step 0068/0087 | Loss 0.0452 | Time 12.4007\n",
      "Epoch 0005 | Step 0085/0087 | Loss 0.0476 | Time 15.5166\n",
      "current acc is 0.8768, best acc is 0.8812\n",
      "time costed = 16.44135s \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. 加载最优模型测试"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "pred_test = predict(model, test_loader, DEVICE)\n",
    "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
    "print(classification_report(y_test, pred_test, digits=4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "11it [00:00, 16.65it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Test Accuracy = 0.8818443804034583 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8728    0.8882    0.8805       340\n",
      "           1     0.8908    0.8757    0.8832       354\n",
      "\n",
      "    accuracy                         0.8818       694\n",
      "   macro avg     0.8818    0.8820    0.8818       694\n",
      "weighted avg     0.8820    0.8818    0.8819       694\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63cbe048cf9cb2a05d6074210765cb6b2c38f366c92a2062bd0ae088a44fc0ea"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('pytorch_gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}